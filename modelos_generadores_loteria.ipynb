{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO GENERADOR DE SEQUENCIAS BASADO EN GAN.\n",
    "Su intención es mostrar ejemplos de modelos generativos para predecir sequencias de numeros segun la condicion de los resultados historicos. Funcionaria como el modelo GAN de generador de imagenes pero basado en sequencias de numeros. Me ayudado de Chat GPT para hacerlo y de un libro de IA de consulta.\n",
    "Realizado en TensorFlow 2 y Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Supongamos que ya tienes cargado el dataframe con los resultados de la lotería\n",
    "\n",
    "df = pd.read_csv('./loteria/Lotoideas.com - Histórico de Resultados - Primitiva - 2013 a 2024.csv')\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "# Limitar los datos a la columna deseada\n",
    "numeros_ganadores = df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6']].to_numpy() \n",
    "\n",
    "# Normalizar los datos\n",
    "numeros_ganadores = numeros_ganadores / 49  # Asumiendo que los números de la lotería son de 1 a 49\n",
    "\n",
    "# Dimensiones\n",
    "LATENT_DIM = 10  # Dimensión del vector latente\n",
    "NUM_FEATURES = numeros_ganadores.shape[1]  # Número de características (números de lotería)\n",
    "\n",
    "# Construir el Generador\n",
    "def build_generator(latent_dim, num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=latent_dim, kernel_initializer='he_normal'),\n",
    "        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(512, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(num_features, activation='sigmoid')  # Sigmoid para escalar entre 0 y 1\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Construir el Discriminador\n",
    "def build_discriminator(num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_dim=num_features, kernel_initializer='he_normal'),\n",
    "        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')  # Salida binaria (real o falso)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instanciar los modelos\n",
    "generator = build_generator(LATENT_DIM, NUM_FEATURES)\n",
    "discriminator = build_discriminator(NUM_FEATURES)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_discriminator = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "optimizer_generator = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "# Compilar el discriminador\n",
    "discriminator.compile(optimizer=optimizer_discriminator, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Congelar el discriminador en el modelo combinado\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Crear el modelo combinado (Generador + Discriminador)\n",
    "gan_input = layers.Input(shape=(LATENT_DIM,))\n",
    "generated_numbers = generator(gan_input)\n",
    "gan_output = discriminator(generated_numbers)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n",
    "# Compilar el modelo combinado\n",
    "gan.compile(optimizer=optimizer_generator, loss='binary_crossentropy')\n",
    "\n",
    "# Preparar los datos reales\n",
    "# Convertir los números ganadores en valores entre 0 y 1\n",
    "batch_size = 32\n",
    "num_epochs = 150\n",
    "\n",
    "# Crear etiquetas para los datos reales y falsos\n",
    "real_labels = tf.ones((batch_size, 1))\n",
    "fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "# Entrenar el GAN\n",
    "for epoch in range(num_epochs):\n",
    "    # Seleccionar una muestra aleatoria de datos reales\n",
    "    idx = np.random.randint(0, numeros_ganadores.shape[0], batch_size)\n",
    "    real_samples = numeros_ganadores[idx]\n",
    "\n",
    "    # Generar datos falsos\n",
    "    noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "    fake_samples = generator.predict(noise)\n",
    "\n",
    "    # Entrenar el discriminador\n",
    "    d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Entrenar el generador (a través del modelo combinado)\n",
    "    noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "    g_loss = gan.train_on_batch(noise, real_labels)  # Usar etiquetas reales para el generador\n",
    "\n",
    "    # Imprimir las pérdidas\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss D: {d_loss[0]:.4f}, Loss G: {g_loss[0]:.4f}\")\n",
    "\n",
    "# Evaluar el modelo generador\n",
    "def generar_numeros_loteria(modelo_generador, num_samples):\n",
    "    noise = np.random.normal(0, 1, (num_samples, LATENT_DIM))\n",
    "    generated_samples = modelo_generador.predict(noise)\n",
    "    return generated_samples * 49  # Desnormalizar para obtener números reales\n",
    "\n",
    "# Generar algunos números de lotería\n",
    "numeros_generados = generar_numeros_loteria(generator, 5)\n",
    "print(\"Números de lotería generados:\")\n",
    "print(np.array(numeros_generados, dtype=np.int32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO DCGAN para GENERADOR de sequencias de numeros basado en un historico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics, losses, optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Cargar datos de lotería desde el archivo CSV\n",
    "csv_path = './loteria/Lotoideas.com - Histórico de Resultados - Primitiva - 2013 a 2024.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Seleccionar las columnas de números ganadores\n",
    "numeros_ganadores = df.iloc[:, 1:7].values\n",
    "\n",
    "# Normalizar los datos (0 a 1) suponiendo que los números van del 1 al 49\n",
    "# Escalar los números a un rango de 0 a 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "numeros_ganadores = scaler.fit_transform(numeros_ganadores)\n",
    "\n",
    "# Convertir los datos a tf.float32\n",
    "numeros_ganadores = tf.convert_to_tensor(numeros_ganadores, dtype=tf.float32)\n",
    "\n",
    "# Dimensiones\n",
    "LATENT_DIM = 4  # Dimensión del vector latente\n",
    "NUM_FEATURES = numeros_ganadores.shape[1]  # Número de características (números de lotería)\n",
    "NOISE_PARAM = 0.1\n",
    "\n",
    "# Crear dataset de TensorFlow\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(numeros_ganadores).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Construir el Generador\n",
    "def build_generator(latent_dim, num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=latent_dim),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(num_features, activation='sigmoid')  # Sigmoid para escalar entre 0 y 1\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Construir el Discriminador\n",
    "def build_discriminator(num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_dim=num_features),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Salida binaria (real o falso)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instanciar los modelos\n",
    "generator = build_generator(LATENT_DIM, NUM_FEATURES)\n",
    "discriminator = build_discriminator(NUM_FEATURES)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_discriminator = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "optimizer_generator = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Modelo DCGAN\n",
    "class DCGAN(models.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.loss_fn = losses.BinaryCrossentropy()  # Usar BinaryCrossentropy\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\n",
    "        self.d_real_acc_metric = metrics.BinaryAccuracy(name=\"d_real_acc\")\n",
    "        self.d_fake_acc_metric = metrics.BinaryAccuracy(name=\"d_fake_acc\")\n",
    "        self.d_acc_metric = metrics.BinaryAccuracy(name=\"d_acc\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "        self.g_acc_metric = metrics.BinaryAccuracy(name=\"g_acc\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.d_loss_metric,\n",
    "            self.d_real_acc_metric,\n",
    "            self.d_fake_acc_metric,\n",
    "            self.d_acc_metric,\n",
    "            self.g_loss_metric,\n",
    "            self.g_acc_metric,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, real_numbers):\n",
    "        # Convertir real_numbers a tf.float32 si es necesario\n",
    "        real_numbers = tf.cast(real_numbers, tf.float32)\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_numbers)[0]\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "       # Train the discriminator on fake images\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_numbers = self.generator(\n",
    "                random_latent_vectors, training=True\n",
    "            )\n",
    "            real_predictions = self.discriminator(real_numbers, training=True)\n",
    "            fake_predictions = self.discriminator(\n",
    "                generated_numbers, training=True\n",
    "            )\n",
    "\n",
    "            real_labels = tf.ones_like(real_predictions)\n",
    "            real_noisy_labels = real_labels + NOISE_PARAM * tf.random.uniform(\n",
    "                tf.shape(real_predictions)\n",
    "            )\n",
    "            fake_labels = tf.zeros_like(fake_predictions)\n",
    "            fake_noisy_labels = fake_labels - NOISE_PARAM * tf.random.uniform(\n",
    "                tf.shape(fake_predictions)\n",
    "            )\n",
    "\n",
    "            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n",
    "            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2.0\n",
    "\n",
    "            g_loss = self.loss_fn(real_labels, fake_predictions)\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            d_loss, self.discriminator.trainable_variables\n",
    "        )\n",
    "        gradients_of_generator = gen_tape.gradient(\n",
    "            g_loss, self.generator.trainable_variables\n",
    "        )\n",
    "\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "        )\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gradients_of_generator, generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.d_real_acc_metric.update_state(real_labels, real_predictions)\n",
    "        self.d_fake_acc_metric.update_state(fake_labels, fake_predictions)\n",
    "        self.d_acc_metric.update_state(\n",
    "            [real_labels, fake_labels], [real_predictions, fake_predictions]\n",
    "        )\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        self.g_acc_metric.update_state(real_labels, fake_predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Instanciar y compilar el modelo DCGAN\n",
    "dcgan = DCGAN(discriminator=discriminator, generator=generator, latent_dim=LATENT_DIM)\n",
    "\n",
    "dcgan.compile(d_optimizer=optimizer_discriminator, g_optimizer=optimizer_generator)\n",
    "\n",
    "# Entrenar el modelo DCGAN\n",
    "EPOCHS = 400\n",
    "dcgan.fit(dataset, epochs = EPOCHS, batch_size=batch_size)\n",
    "\"\"\"\n",
    "for epoch in range(EPOCHS):\n",
    "    for real_numbers in dataset:\n",
    "        metrics = dcgan.train_step(real_numbers)\n",
    "\n",
    "    # Imprimir métricas de entrenamiento\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
    "            f\"Loss D: {metrics['d_loss']:.4f}, \"\n",
    "            f\"Acc Real D: {metrics['d_real_acc']:.4f}, \"\n",
    "            f\"Acc Fake D: {metrics['d_fake_acc']:.4f}, \"\n",
    "            f\"Loss G: {metrics['g_loss']:.4f}, \"\n",
    "            f\"Acc G: {metrics['g_acc']:.4f}\"\n",
    "        )\n",
    "\"\"\"\n",
    "# Generar números de lotería después de entrenar\n",
    "def generar_numeros_loteria(modelo_generador, num_samples):\n",
    "    noise = np.random.normal(0, 1, (num_samples, LATENT_DIM))\n",
    "    generated_samples = modelo_generador.predict(noise)\n",
    "    return np.round(generated_samples * 49).astype(int)  # Desnormalizar y redondear\n",
    "\n",
    "# Generar algunos números de lotería\n",
    "numeros_generados = generar_numeros_loteria(generator, 5)\n",
    "print(\"Números de lotería generados:\")\n",
    "print(numeros_generados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO WGAN_GP para GENERACION de sequencias de numeros basado en un historico previo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics, optimizers\n",
    "\n",
    "# Cargar datos de lotería desde el archivo CSV\n",
    "csv_path = './loteria/Lotoideas.com - Histórico de Resultados - Primitiva - 2013 a 2024.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Seleccionar las columnas de números ganadores\n",
    "numeros_ganadores = df.iloc[:, 1:7].values\n",
    "\n",
    "# Normalizar los datos (0 a 1) suponiendo que los números van del 1 al 49\n",
    "numeros_ganadores = numeros_ganadores / 49.0\n",
    "\n",
    "# Convertir los datos a tf.float32\n",
    "numeros_ganadores = tf.convert_to_tensor(numeros_ganadores, dtype=tf.float32)\n",
    "\n",
    "# Dimensiones\n",
    "LATENT_DIM = 2 # Dimensión del vector latente\n",
    "NUM_FEATURES = numeros_ganadores.shape[1]  # Número de características (números de lotería)\n",
    "\n",
    "# Crear dataset de TensorFlow\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(numeros_ganadores).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Construir el Generador\n",
    "def build_generator(latent_dim, num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=latent_dim),        \n",
    "        layers.Dense(256, activation='relu', kernel_initializer=\"glorot_uniform\"),        \n",
    "        layers.Dense(512, activation='relu', kernel_initializer=\"glorot_uniform\"),        \n",
    "        layers.Dense(num_features, activation='sigmoid')  # Sigmoid para escalar entre 0 y 1\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Construir el Discriminador\n",
    "def build_discriminator(num_features):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_dim=num_features),\n",
    "        layers.Dense(256, activation='relu', kernel_initializer=\"glorot_uniform\"),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=\"glorot_uniform\"),\n",
    "        layers.Dense(1)  # Salida de un solo valor para la función de pérdida Wasserstein\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instanciar los modelos\n",
    "generator = build_generator(LATENT_DIM, NUM_FEATURES)\n",
    "discriminator = build_discriminator(NUM_FEATURES)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_discriminator = optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "optimizer_generator = optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "\n",
    "# Pérdida Wasserstein\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "# Penalización del Gradiente\n",
    "def gradient_penalty(real_numbers, fake_numbers, batch_size):\n",
    "    # Calcula el gradiente de interpolación\n",
    "    alpha = tf.random.normal([batch_size, 1], 0.0, 1.0)\n",
    "    diff = fake_numbers - real_numbers\n",
    "    interpolated = real_numbers + alpha * diff\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated)\n",
    "\n",
    "    grads = tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    penalty = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return penalty\n",
    "\n",
    "# Modelo WGAN-GP\n",
    "class WGAN_GP(models.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim, critic_steps, gradient_penalty_weight):\n",
    "        super(WGAN_GP, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gradient_penalty_weight = gradient_penalty_weight\n",
    "        self.critic_steps = critic_steps\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(WGAN_GP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_wass_loss_metric = metrics.Mean(name=\"c_wass_loss\")\n",
    "        self.c_gp_metric = metrics.Mean(name=\"c_gp\")\n",
    "        self.c_loss_metric = metrics.Mean(name=\"c_loss\")\n",
    "        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.c_loss_metric,\n",
    "            self.c_wass_loss_metric,\n",
    "            self.c_gp_metric,\n",
    "            self.g_loss_metric,\n",
    "        ]\n",
    "    \"\"\"\n",
    "    def train_step(self, real_numbers):\n",
    "        # Convertir real_numbers a tf.float32 si es necesario\n",
    "        real_numbers = tf.cast(real_numbers, tf.float32)\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_numbers)[0]\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Generate fake numbers\n",
    "        fake_numbers = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "        # Train the discriminator on fake and real numbers\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            real_predictions = self.discriminator(real_numbers, training=True)\n",
    "            fake_predictions = self.discriminator(fake_numbers, training=True)\n",
    "\n",
    "            # Calculate discriminator loss\n",
    "            d_loss_real = wasserstein_loss(tf.ones_like(real_predictions), real_predictions)\n",
    "            d_loss_fake = wasserstein_loss(tf.zeros_like(fake_predictions), fake_predictions)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\n",
    "                    real_predictions\n",
    "                )\n",
    "            c_gp = self.gradient_penalty(\n",
    "                    batch_size, real_numbers, fake_numbers\n",
    "                    )\n",
    "                \n",
    "            c_loss = c_wass_loss + c_gp * self.gp_weight\n",
    "\n",
    "            # Add gradient penalty\n",
    "            penalty = gradient_penalty(real_numbers, fake_numbers, batch_size)\n",
    "            d_loss += self.gradient_penalty_weight * penalty\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            d_loss, self.discriminator.trainable_variables\n",
    "        )\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(gradients_of_discriminator, self.discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake_predictions_for_gen = self.discriminator(\n",
    "                self.generator(random_latent_vectors, training=True), training=True\n",
    "            )\n",
    "            g_loss = wasserstein_loss(tf.ones_like(fake_predictions_for_gen), fake_predictions_for_gen)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(\n",
    "            g_loss, self.generator.trainable_variables\n",
    "        )\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gradients_of_generator, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \"\"\"\n",
    "    def gradient_penalty(self, batch_size, real_samples, fake_samples):\n",
    "        \"\"\"\n",
    "        Computes the gradient penalty for the Wasserstein GAN.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size: Integer, the number of samples in each batch.\n",
    "        - real_samples: Tensor, the real data samples.\n",
    "        - fake_samples: Tensor, the generated data samples.\n",
    "\n",
    "        Returns:\n",
    "        - gp: Tensor, the computed gradient penalty.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate random interpolation coefficients\n",
    "        alpha = tf.random.uniform([batch_size, 1], minval=0.0, maxval=1.0)\n",
    "        \n",
    "        # Interpolate between real and fake samples\n",
    "        interpolated_samples = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "        \n",
    "        # Watch the interpolated samples for gradient computation\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_samples)\n",
    "            # Forward pass through the discriminator\n",
    "            interpolated_predictions = self.discriminator(interpolated_samples, training=True)\n",
    "        \n",
    "        # Compute the gradients with respect to the interpolated samples\n",
    "        gradients = gp_tape.gradient(interpolated_predictions, [interpolated_samples])[0]\n",
    "        \n",
    "        # Compute the L2 norm of the gradients for each sample\n",
    "        gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]))\n",
    "        \n",
    "        # Compute the mean squared deviation of the L2 norms from 1\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n",
    "        \n",
    "        return gradient_penalty\n",
    "\n",
    "    def train_step(self, real_numbers):\n",
    "        # Convertir real_numbers a tf.float32 si es necesario\n",
    "        real_numbers = tf.cast(real_numbers, tf.float32)\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_numbers)[0]\n",
    "        \n",
    "        for i in range(self.critic_steps):\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim), dtype=tf.float32\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_numbers = self.generator(\n",
    "                    random_latent_vectors, training=True\n",
    "                )\n",
    "                fake_predictions = self.discriminator(fake_numbers, training=True)\n",
    "                real_predictions = self.discriminator(real_numbers, training=True)\n",
    "\n",
    "                c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\n",
    "                    real_predictions\n",
    "                )\n",
    "                c_gp = self.gradient_penalty(\n",
    "                    batch_size, real_numbers, fake_numbers\n",
    "                )\n",
    "                c_loss = c_wass_loss + c_gp * self.gradient_penalty_weight\n",
    "\n",
    "            c_gradient = tape.gradient(c_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(c_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_numbers = self.generator(random_latent_vectors, training=True)\n",
    "            fake_predictions = self.discriminator(fake_numbers, training=True)\n",
    "            g_loss = -tf.reduce_mean(fake_predictions)\n",
    "\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        self.c_loss_metric.update_state(c_loss)\n",
    "        self.c_wass_loss_metric.update_state(c_wass_loss)\n",
    "        self.c_gp_metric.update_state(c_gp)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Instanciar y compilar el modelo WGAN-GP\n",
    "wgan_gp = WGAN_GP(discriminator=discriminator, generator=generator, latent_dim=LATENT_DIM, gradient_penalty_weight=10.0, critic_steps=1)\n",
    "\n",
    "wgan_gp.compile(d_optimizer=optimizer_discriminator, g_optimizer=optimizer_generator)\n",
    "\n",
    "# Entrenar el modelo WGAN-GP\n",
    "EPOCHS = 200\n",
    "\"\"\"\n",
    "for epoch in range(EPOCHS):\n",
    "    for real_numbers in dataset:\n",
    "        metrics = wgan_gp.train_step(real_numbers)\n",
    "\n",
    "    # Imprimir métricas de entrenamiento\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
    "            f\"Loss C: {metrics['c_loss']:.4f}, \"\n",
    "            f\"Loss G: {metrics['g_loss']:.4f}\"\n",
    "        )\n",
    "\"\"\"\n",
    "wgan_gp.fit(dataset, epochs=EPOCHS, batch_size=batch_size)\n",
    "\n",
    "# Generar números de lotería después de entrenar\n",
    "def generar_numeros_loteria(modelo_generador, num_samples):\n",
    "    noise = np.random.normal(0, 1, (num_samples, LATENT_DIM))\n",
    "    generated_samples = modelo_generador.predict(noise)\n",
    "    return np.round(generated_samples * 49).astype(int)  # Desnormalizar y redondear\n",
    "\n",
    "# Generar algunos números de lotería\n",
    "numeros_generados = generar_numeros_loteria(generator, 5)\n",
    "print(\"Números de lotería generados:\")\n",
    "print(numeros_generados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO VAE para generacion de numeros basado en un historico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics, losses\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Carga y Preprocesamiento del Dataset\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "file_path = './loteria/Lotoideas.com - Histórico de Resultados - Primitiva - 2013 a 2024.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extraer las columnas que contienen los números\n",
    "columns_of_interest = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6']\n",
    "sequences = df[columns_of_interest].values\n",
    "\n",
    "# Normalizar los números entre 0 y 1\n",
    "scaler = MinMaxScaler()\n",
    "sequences_normalized = scaler.fit_transform(sequences)\n",
    "# Crear dataset de TensorFlow\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sequences_normalized).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 2. Definición del Encoder y Decoder\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Layer to sample from the latent space.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(input_dim, latent_dim, hidden_dims):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    for dim in hidden_dims:\n",
    "        x = Dense(dim, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    z_mean = Dense(latent_dim, kernel_initializer='he_normal')(x)\n",
    "    z_log_var = Dense(latent_dim, kernel_initializer='he_normal')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    return Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "def build_decoder(latent_dim, output_dim, hidden_dims):\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    x = latent_inputs\n",
    "    for dim in hidden_dims[::-1]:\n",
    "        x = Dense(dim, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    outputs = Dense(output_dim, activation='sigmoid', kernel_initializer='he_normal')(x)\n",
    "    return Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "# 3. Definición de la clase LotteryVAE utilizando la clase VAE proporcionada\n",
    "\n",
    "class LotteryVAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(LotteryVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data, training=True)\n",
    "            reconstruction_loss = tf.reduce_mean(losses.mse(data, reconstruction))\n",
    "            kl_loss = tf.reduce_mean(\n",
    "                -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(losses.mse(data, reconstruction))\n",
    "        kl_loss = tf.reduce_mean(\n",
    "            -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "# 4. Construcción del VAE, Entrenamiento y Generación\n",
    "\n",
    "input_dim = sequences_normalized.shape[1]\n",
    "latent_dim = 10\n",
    "hidden_dims = [64, 32]\n",
    "\n",
    "# Crear encoder y decoder\n",
    "encoder = build_encoder(input_dim, latent_dim, hidden_dims)\n",
    "decoder = build_decoder(latent_dim, input_dim, hidden_dims)\n",
    "\n",
    "# Crear la instancia del VAE\n",
    "vae = LotteryVAE(encoder, decoder)\n",
    "\n",
    "# Compilar el modelo\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Entrenar el VAE\n",
    "vae.fit(dataset, epochs=100, batch_size=batch_size)\n",
    "\n",
    "# 5. Generación de Nuevas Secuencias de Números de Lotería\n",
    "\n",
    "def generate_lottery_numbers(vae, scaler, num_samples=1):\n",
    "    latent_sample = np.random.normal(size=(num_samples, latent_dim))\n",
    "    generated = vae.decoder.predict(latent_sample)\n",
    "    return np.round(scaler.inverse_transform(generated)).astype(int)\n",
    "\n",
    "generated_sequences = generate_lottery_numbers(vae, scaler, num_samples=5)\n",
    "print(\"Secuencias generadas:\\n\", generated_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO EBM para GENERACION de sequencias basado en un historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, metrics, optimizers\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Parámetros globales\n",
    "SEQUENCE_LENGTH = 6  # 6 números en cada secuencia de lotería\n",
    "LATENT_DIM = 32  # Dimensión del espacio latente\n",
    "STEPS = 10  # Pasos para la generación\n",
    "STEP_SIZE = 0.1  # Tamaño de paso para la dinámica\n",
    "NOISE = 0.01  # Ruido aplicado\n",
    "ALPHA = 0.1  # Regularización\n",
    "BUFFER_SIZE = 1000  # Tamaño del buffer\n",
    "BATCH_SIZE = 32  # Tamaño de batch\n",
    "\n",
    "# 1. Cargar y normalizar los datos de lotería\n",
    "file_path = './loteria/Lotoideas.com - Histórico de Resultados - Primitiva - 2013 a 2024.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "columns_of_interest = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6']\n",
    "sequences = df[columns_of_interest].values\n",
    "\n",
    "# Asegurar que el número de secuencias sea par\n",
    "if np.shape(sequences)[0] % 2 != 0:\n",
    "    sequences = np.vstack([sequences, sequences[-1]])  # Duplicar la última secuencia\n",
    "\n",
    "# Normalizar los números entre -1 y 1\n",
    "scaler = MinMaxScaler((-1, 1))\n",
    "sequences_normalized = scaler.fit_transform(sequences)\n",
    "\n",
    "# 2. Definir la arquitectura del EBM\n",
    "ebm_input = Input(shape=(SEQUENCE_LENGTH,))\n",
    "x = Dense(64, activation='relu')(ebm_input)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "ebm_output = Dense(1)(x)\n",
    "model = models.Model(ebm_input, ebm_output)\n",
    "model.summary()\n",
    "\n",
    "# 3. Definir la función de generación de secuencias usando Langevin Dynamics\n",
    "@tf.function\n",
    "def generate_samples(model, inp_seqs, steps, step_size, noise, return_seq_per_step=False):\n",
    "    seqs_per_step = []\n",
    "    for _ in range(steps):\n",
    "        inp_seqs += tf.random.normal(inp_seqs.shape, mean=0, stddev=noise)\n",
    "        inp_seqs = tf.clip_by_value(inp_seqs, -1.0, 1.0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inp_seqs)\n",
    "            out_score = model(inp_seqs)\n",
    "        grads = tape.gradient(out_score, inp_seqs)\n",
    "        inp_seqs += step_size * grads\n",
    "        inp_seqs = tf.clip_by_value(inp_seqs, -1.0, 1.0)\n",
    "        if return_seq_per_step:\n",
    "            seqs_per_step.append(inp_seqs)\n",
    "    if return_seq_per_step:\n",
    "        return tf.stack(seqs_per_step, axis=0)\n",
    "    return inp_seqs\n",
    "\n",
    "# 4. Buffer de ejemplos\n",
    "class Buffer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.examples = [tf.random.uniform(shape=(1, SEQUENCE_LENGTH)) for _ in range(BATCH_SIZE)]\n",
    "    \n",
    "    @tf.function\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
    "        rand_seqs = tf.random.uniform((n_new, SEQUENCE_LENGTH))\n",
    "        old_seqs = tf.concat(\n",
    "            [self.examples[i] for i in np.random.choice(len(self.examples), BATCH_SIZE - n_new)], axis=0\n",
    "        )\n",
    "        inp_seqs = tf.concat([rand_seqs, old_seqs], axis=0)\n",
    "        inp_seqs = generate_samples(\n",
    "            self.model, inp_seqs, steps=steps, step_size=step_size, noise=noise\n",
    "        )\n",
    "        self.examples = tf.split(inp_seqs, BATCH_SIZE, axis=0) + self.examples[:BUFFER_SIZE - BATCH_SIZE]\n",
    "        return inp_seqs\n",
    "\n",
    "# 5. Definir la clase EBM\n",
    "class LotteryEBM(models.Model):\n",
    "    def __init__(self):\n",
    "        super(LotteryEBM, self).__init__()\n",
    "        self.model = model\n",
    "        self.buffer = Buffer(self.model)\n",
    "        self.alpha = ALPHA\n",
    "        self.loss_metric = metrics.Mean(name=\"loss\")\n",
    "        self.reg_loss_metric = metrics.Mean(name=\"reg\")\n",
    "        self.cdiv_loss_metric = metrics.Mean(name=\"cdiv\")\n",
    "        self.real_out_metric = metrics.Mean(name=\"real\")\n",
    "        self.fake_out_metric = metrics.Mean(name=\"fake\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.loss_metric,\n",
    "            self.reg_loss_metric,\n",
    "            self.cdiv_loss_metric,\n",
    "            self.real_out_metric,\n",
    "            self.fake_out_metric,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, real_seqs):\n",
    "        real_seqs = tf.convert_to_tensor(real_seqs)\n",
    "        real_seqs += tf.random.normal(shape=tf.shape(real_seqs), mean=0, stddev=NOISE)\n",
    "        real_seqs = tf.clip_by_value(real_seqs, -1.0, 1.0)\n",
    "        fake_seqs = self.buffer.sample_new_exmps(steps=STEPS, step_size=STEP_SIZE, noise=NOISE)\n",
    "               \n",
    "        inp_seqs = tf.concat([real_seqs, fake_seqs], axis=0)\n",
    "       \n",
    "        with tf.GradientTape() as training_tape:\n",
    "            real_out, fake_out = tf.split(self.model(inp_seqs), num_or_size_splits=2, axis=0)\n",
    "            cdiv_loss = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(real_out, axis=0)\n",
    "            reg_loss = self.alpha * tf.reduce_mean(real_out**2 + fake_out**2, axis=0)\n",
    "            loss = cdiv_loss + reg_loss\n",
    "        \n",
    "        grads = training_tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.reg_loss_metric.update_state(reg_loss)\n",
    "        self.cdiv_loss_metric.update_state(cdiv_loss)\n",
    "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
    "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, real_seqs):\n",
    "        batch_size = real_seqs.shape[0]\n",
    "        fake_seqs = tf.random.uniform((batch_size, SEQUENCE_LENGTH))\n",
    "        inp_seqs = tf.concat([real_seqs, fake_seqs], axis=0)\n",
    "\n",
    "        # Asegurando que tengamos un número par de secuencias para dividir\n",
    "        total_seqs = tf.shape(inp_seqs)[0]\n",
    "        if total_seqs % 2 != 0:\n",
    "            # Duplicar la última secuencia si el total es impar\n",
    "            last_seq = tf.expand_dims(inp_seqs[-1], axis=0)\n",
    "            inp_seqs = tf.concat([inp_seqs, last_seq], axis=0)\n",
    "        \n",
    "        real_out, fake_out = tf.split(self.model(inp_seqs), num_or_size_splits=2, axis=0)\n",
    "        cdiv = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(real_out, axis=0)\n",
    "        self.cdiv_loss_metric.update_state(cdiv)\n",
    "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
    "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
    "        return {m.name: m.result() for m in self.metrics[2:]}\n",
    "\n",
    "# 6. Compilación y entrenamiento del EBM\n",
    "ebm = LotteryEBM()\n",
    "ebm.compile(optimizer=optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Convertir las secuencias normalizadas en tensores de entrenamiento\n",
    "x_train = tf.convert_to_tensor(sequences_normalized, dtype=tf.float32)\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n",
    "\n",
    "# Entrenamiento\n",
    "ebm.fit(x_train, epochs=1000)\n",
    "\n",
    "# 7. Generación de nuevas secuencias de lotería determinísticas\n",
    "def generate_lottery_numbers(ebm, num_samples=5):\n",
    "    latent_sample = tf.random.uniform((num_samples, SEQUENCE_LENGTH))\n",
    "    generated = generate_samples(ebm.model, latent_sample, steps=STEPS, step_size=STEP_SIZE, noise=NOISE)\n",
    "    return np.round(scaler.inverse_transform(generated)).astype(int)\n",
    "\n",
    "generated_sequences = generate_lottery_numbers(ebm, num_samples=5)\n",
    "print(\"Secuencias generadas:\\n\", generated_sequences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
